{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca3eda6-e2e9-4cf9-bc9d-f832241ab281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practice Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "add5cd37-f1e8-4888-a10f-f2d482155eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\irfan\\anaconda3\\lib\\site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c632ff6-f091-47d3-a56d-10e65aba999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa33b8c8-db6e-432f-ac9d-ff76b6579268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bbf4076-5532-4b62-9353-91edcea584dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Google Gemini: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini:\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", temperature = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e879707-36e2-49cf-9e4c-c2598fb57e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6032b404-5c41-4e02-ab1b-825abdc489af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b361a06-1327-498b-ba61-e8136b04248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Hello, tell me a straightforward story\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatGoogleGenerativeAI] [2.61s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.0-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The old clockmaker, Silas, had lived in the dusty attic workshop for seventy years. His hands, gnarled like ancient branches, moved with practiced ease, coaxing life back into broken gears and shattered faces. One day, a young girl named Lily wandered into his shop, lost and wide-eyed. Her own clock, a cheap plastic one shaped like a cartoon cat, had stopped ticking.\\n\\nSilas smiled, his eyes crinkling at the corners. \\\"Broken, is it?\\\" he asked, his voice raspy. Lily nodded, clutching the cat clock tightly.\\n\\nSilas examined it with a gentle curiosity. It was unlike anything he usually worked on, but he saw the importance it held for her. He cleaned the battery terminals, tightened a loose connection, and in moments, the cat clock blinked its plastic eyes and purred softly.\\n\\nLily's face lit up. She handed Silas a crumpled dollar bill, which he politely refused. \\\"The best payment,\\\" he said, \\\"is knowing you can tell the time again.\\\"\\n\\nLily smiled, clutching her purring cat clock, and skipped out of the shop. Silas returned to his workbench, a faint smile playing on his lips. The workshop, usually filled with the ticking of antique clocks, was for a brief moment, also filled with the quiet echo of a happy girl. He continued his work, knowing that even the smallest repair could bring a little bit of joy to the world.\",\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.0-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--071234e2-f288-4cdf-b87a-a5a58ec8b991-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 7,\n",
      "              \"output_tokens\": 295,\n",
      "              \"total_tokens\": 302,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        },\n",
      "        \"text\": \"The old clockmaker, Silas, had lived in the dusty attic workshop for seventy years. His hands, gnarled like ancient branches, moved with practiced ease, coaxing life back into broken gears and shattered faces. One day, a young girl named Lily wandered into his shop, lost and wide-eyed. Her own clock, a cheap plastic one shaped like a cartoon cat, had stopped ticking.\\n\\nSilas smiled, his eyes crinkling at the corners. \\\"Broken, is it?\\\" he asked, his voice raspy. Lily nodded, clutching the cat clock tightly.\\n\\nSilas examined it with a gentle curiosity. It was unlike anything he usually worked on, but he saw the importance it held for her. He cleaned the battery terminals, tightened a loose connection, and in moments, the cat clock blinked its plastic eyes and purred softly.\\n\\nLily's face lit up. She handed Silas a crumpled dollar bill, which he politely refused. \\\"The best payment,\\\" he said, \\\"is knowing you can tell the time again.\\\"\\n\\nLily smiled, clutching her purring cat clock, and skipped out of the shop. Silas returned to his workbench, a faint smile playing on his lips. The workshop, usually filled with the ticking of antique clocks, was for a brief moment, also filled with the quiet echo of a happy girl. He continued his work, knowing that even the smallest repair could bring a little bit of joy to the world.\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The old clockmaker, Silas, had lived in the dusty attic workshop for seventy years. His hands, gnarled like ancient branches, moved with practiced ease, coaxing life back into broken gears and shattered faces. One day, a young girl named Lily wandered into his shop, lost and wide-eyed. Her own clock, a cheap plastic one shaped like a cartoon cat, had stopped ticking.\\n\\nSilas smiled, his eyes crinkling at the corners. \"Broken, is it?\" he asked, his voice raspy. Lily nodded, clutching the cat clock tightly.\\n\\nSilas examined it with a gentle curiosity. It was unlike anything he usually worked on, but he saw the importance it held for her. He cleaned the battery terminals, tightened a loose connection, and in moments, the cat clock blinked its plastic eyes and purred softly.\\n\\nLily\\'s face lit up. She handed Silas a crumpled dollar bill, which he politely refused. \"The best payment,\" he said, \"is knowing you can tell the time again.\"\\n\\nLily smiled, clutching her purring cat clock, and skipped out of the shop. Silas returned to his workbench, a faint smile playing on his lips. The workshop, usually filled with the ticking of antique clocks, was for a brief moment, also filled with the quiet echo of a happy girl. He continued his work, knowing that even the smallest repair could bring a little bit of joy to the world.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--071234e2-f288-4cdf-b87a-a5a58ec8b991-0', usage_metadata={'input_tokens': 7, 'output_tokens': 295, 'total_tokens': 302, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(\"Hello, tell me a straightforward story\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f076e-84c4-4d96-a7cd-543371819603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1def127-9cec-4b9f-8e93-f73c0184dfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The old woman, Elara, lived in a small cottage at the edge of the woods. Every morning, she woke with the sun, made herself a cup of strong tea, and walked into the forest. She wasn't looking for anything in particular, just enjoying the quiet rustle of leaves and the songs of the birds.\\n\\nOne day, she stumbled upon a small, injured bird. Its wing was bent at an unnatural angle. Elara carefully scooped it up, brought it back to her cottage, and splinted its wing with twigs and cloth. She fed it seeds and water, and kept it warm.\\n\\nFor weeks, Elara cared for the bird. Slowly, its wing healed. Finally, the day came when she took it back to the edge of the woods. She held it in her hand, felt its tiny heart beating fast, and then gently tossed it into the air.\\n\\nThe bird hesitated for a moment, then soared upwards, circling once above Elara's head before disappearing into the trees. Elara smiled, a deep, contented smile. She knew she might never see the bird again, but she had done what she could. And that was enough. She turned and walked back to her cottage, the quiet rustle of leaves now sounding like a song of gratitude.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--0f65916d-9209-4806-a00f-b0dd38e6d0a1-0', usage_metadata={'input_tokens': 7, 'output_tokens': 265, 'total_tokens': 272, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello, tell me a straightforward story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812dae4-5d40-4038-b51f-9ed0f95f30f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2f76e-3100-4452-87af-480ebf90c4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4eb09bf4-141f-4383-8a0b-703ab300dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes:\n",
    "\n",
    "#The Runnable interface is the foundation for working with LangChain components\n",
    "\n",
    "#Invoked: A single input is transformed into an output.\n",
    "#Batched: Multiple inputs are efficiently transformed into outputs.\n",
    "#Streamed: Outputs are streamed as they are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "110134cd-84cf-49b1-8103-619fdd05e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec18fe57-3677-40aa-b4af-c3068c718ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f49efc6f-5d7f-437e-81c3-2d29d2faad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are the best comedian to exist\"), \n",
    "                                                (\"human\", \"Tell me a joke about {topic}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a938e05-9fad-4b7a-bf23-abc5fa49b1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"topic\": \"plants\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are the best comedian to exist', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about plants', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_prompt.invoke({\"topic\":\"plants\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1650474e-0686-45ea-aa12-4de232b01cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You construct a prompt template consisting of templates for a SystemMessage and a HumanMessage using from_messages.\n",
    "# You can think of SystemMessages as meta-instructions that are not part of the current conversation, but purely guide input.\n",
    "# The prompt template contains {topic} in curly braces. This denotes a required parameter named \"topic\".\n",
    "# You invoke the prompt template with a dict with a key named \"topic\" and a value \"beets\".\n",
    "# The result contains the formatted messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edd6ac-709d-468a-a296-2140e21dc78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "452c9b5e-ba1b-4feb-8c00-25da04876d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can compose Runnables into “chains” using the pipe \n",
    "# (|) operator where you .invoke() the next step with the output of \n",
    "# the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d88af5e6-0347-48ff-937c-a71ecae1518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = joke_prompt | model\n",
    "\n",
    "# Above is itself a Runnable and AUTOMATICALLY implements .invoke() \n",
    "\n",
    "# This is the foundation of LangChain Expression Language (LCEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b694e379-eb3f-4c8f-95d4-7b6426f976a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = joke_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "af44289a-8dd8-4290-b505-436971062bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the bicycle fall over?\\n\\nBecause it was two-tired! \\n\\n...\\n\\n...Okay, okay, you're right, that wasn't about *plants.* My bad. Let me try again:\\n\\nWhy did the tomato blush?\\n\\nBecause it saw the salad dressing!\\n\\n...\\n\\nStill not plant-y enough? Fine, you asked for it:\\n\\nWhat do you call a sad strawberry?\\n\\nA blueberry.\\n\\n...\\n\\n...You know, maybe my material needs some…watering. Ba dum tss!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--ea4e7001-d625-4678-a3b7-7e34d6a417a4-0', usage_metadata={'input_tokens': 13, 'output_tokens': 111, 'total_tokens': 124, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"plants\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9287840-8c6f-4c76-8714-81fd909324b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "481d2716-172a-4ab9-a8ad-a4397ecc0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "079eac9d-22d7-48cd-8122-d5ae6f12ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain = chain | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5cbd1e68-8c59-446d-8908-ce43a6e9d2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the bicycle fall over?\\n\\n... Because it was two TIRED!\\n\\nOkay, okay, I know, I know, a little corny. But listen...\\n\\nWhat do you call a sad strawberry?\\n\\n... A blueberry!\\n\\nAlright, alright, I'm warming up!  Hold on...\\n\\nWhat's a plant's favorite subject in school?\\n\\n... Algebra!  They love square roots!\\n\\nBoom! How was that?  You like my growing sense of humor?  Don't leaf me hanging! Let me know if you want another one!\""
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chain.invoke({\"topic\" : \"plants\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71ce2d-2012-472d-86f0-1a784be68992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153ca3e-47f3-4137-bb8d-caed0540bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Runnables implement the .stream() method (and .astream() if you’re working in async environments), including chains. \n",
    "# This method returns a generator that will yield output as soon as it’s available, \n",
    "# which allows us to get output as quickly as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52bf5bc-cf62-491c-a812-575c624acee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIMITATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1b9ca-35f5-482d-8229-5721dcce7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While every Runnable implements .stream(), not all of them support multiple chunks. \n",
    "# For example, if I call .stream() on a Prompt Template, \n",
    "# it will just yield a single chunk with the same output as .invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19c8ae-9d41-4e82-9d58-5022afa341fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28354aee-554b-47bc-a2d8-f53b8e0a6e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"topic\": \"plant\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are the best comedian to exist\\nHuman: Tell me a joke about plant\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "Why| did| the tomato blush? \n",
      "\n",
      "Because it saw the salad dressing!\n",
      "\n",
      "... I|'m here all week, folks! Try the veal! (Actually, on| second thought, maybe just stick with the lettuce... safer bet, ya know?) Ba-dum-tiss!\n",
      "|\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [696ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"generation_info\": {\n",
      "          \"safety_ratings\": [],\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.0-flash\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Why did the tomato blush? \\n\\nBecause it saw the salad dressing!\\n\\n... I'm here all week, folks! Try the veal! (Actually, on second thought, maybe just stick with the lettuce... safer bet, ya know?) Ba-dum-tiss!\\n\",\n",
      "            \"response_metadata\": {\n",
      "              \"safety_ratings\": [],\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.0-flash\"\n",
      "            },\n",
      "            \"type\": \"AIMessageChunk\",\n",
      "            \"id\": \"run--1b6576b9-03f5-4051-9dee-64e5469d1624\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 13,\n",
      "              \"output_tokens\": 57,\n",
      "              \"total_tokens\": 70,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        },\n",
      "        \"text\": \"Why did the tomato blush? \\n\\nBecause it saw the salad dressing!\\n\\n... I'm here all week, folks! Try the veal! (Actually, on second thought, maybe just stick with the lettuce... safer bet, ya know?) Ba-dum-tiss!\\n\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [403ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Why did the tomato blush? \\n\\nBecause it saw the salad dressing!\\n\\n... I'm here all week, folks! Try the veal! (Actually, on second thought, maybe just stick with the lettuce... safer bet, ya know?) Ba-dum-tiss!\\n\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [716ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Why did the tomato blush? \\n\\nBecause it saw the salad dressing!\\n\\n... I'm here all week, folks! Try the veal! (Actually, on second thought, maybe just stick with the lettuce... safer bet, ya know?) Ba-dum-tiss!\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for chunks in new_chain.stream({\"topic\": \"plant\"}):\n",
    "    print(chunks, end = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a70a03-57dc-48da-81c3-ef98993ad7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains composed like new_chain will start streaming as early as possible\n",
    "# which in this case is the Chat Model in the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593c40e-2cd7-4d1c-b18d-736fb38123c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4e93d-88a2-4192-ba53-261bc46ec814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coming to RAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81f11eec-07b1-481f-ac9b-38367e257286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is the date today, right now\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatGoogleGenerativeAI] [450ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.0-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Okay, the date today is October 27, 2023.\",\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.0-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--6f9f185c-bb75-4cda-a7f9-2fa08e524ec4-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 8,\n",
      "              \"output_tokens\": 18,\n",
      "              \"total_tokens\": 26,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        },\n",
      "        \"text\": \"Okay, the date today is October 27, 2023.\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, the date today is October 27, 2023.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--6f9f185c-bb75-4cda-a7f9-2fa08e524ec4-0', usage_metadata={'input_tokens': 8, 'output_tokens': 18, 'total_tokens': 26, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the date today, right now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "31d55f44-be36-4522-b405-6196c2ab8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^ The above is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404badf-fc3c-4a55-b12b-9977fabf9d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e156f087-66d7-4e9c-92d0-f1df133318b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dbb2a05-64a9-441a-90ad-e05c245ba920",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you know the current date is {curr_date}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ab42e35-123f-4efd-a709-941a4936681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: you know the current date is 2025-06-07\\nHuman: What is the current date\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [473ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.0-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The current date is 2025-06-07.\",\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.0-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--6cd8a49a-8267-42cb-a374-15cfd2629ade-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 22,\n",
      "              \"output_tokens\": 17,\n",
      "              \"total_tokens\": 39,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        },\n",
      "        \"text\": \"The current date is 2025-06-07.\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The current date is 2025-06-07.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [479ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The current date is 2025-06-07.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current date is 2025-06-07.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"curr_date\" : date.today(),\n",
    "    \"question\" : \"What is the current date\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5771e14-c7e4-49dc-aa3b-4bdcb419066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NICEEEEEEEEEEEEEEEEEEEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821f6df-a75a-4886-8136-0b0895ff41be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcbafa-985b-41ac-b0dc-be47489cf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain has a set_debug() method that will return more granular logs of the chain internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97364415-a542-4ef8-9c9e-ace1ebd0b5ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c462f53-1f0e-4873-bf4d-94e63813f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015655f6-7c69-496f-a801-5c941e017f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b338e9-46b2-4a7c-886b-652bb2e19b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After importing this ^^\n",
    "# I had to rerun all the cells and every output gave me a detailed stepwise LLM in and out -> put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8309cf45-c4ec-4527-a20e-fb38ae488ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: you know the current date is 2025-06-07\\nHuman: What is the current date\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [428ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.0-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The current date is 2025-06-07.\",\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.0-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--1424fb6f-42fa-4631-a188-e43e00c538a0-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 22,\n",
      "              \"output_tokens\": 17,\n",
      "              \"total_tokens\": 39,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        },\n",
      "        \"text\": \"The current date is 2025-06-07.\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The current date is 2025-06-07.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [431ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The current date is 2025-06-07.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current date is 2025-06-07.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_debug(True)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"curr_date\" : date.today(),\n",
    "    \"question\" : \"What is the current date\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd3328-3885-406c-beb4-491dfcc1935f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa09e0-c36c-4505-9307-c420b6504242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the astream_events() method to return this data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
